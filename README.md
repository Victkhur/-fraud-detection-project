# Fraud Detection using Anomaly Detection & XGBoost

## ğŸ“Œ Project Overview
Fraud detection is a critical challenge in the financial industry due to the high imbalance between fraudulent and legitimate transactions. This project explores **Anomaly Detection** techniques, including **Isolation Forest, Autoencoders, and XGBoost with SMOTE**, to identify fraudulent transactions effectively.

## ğŸ“‚ Dataset
The dataset includes information about transactions made by credit cards in September 2013 by European cardholders. I will use the trained model to assess if a credit card transaction is legitimate or not. The dataset contains **284,807** transactions with the following distribution:
- **Legitimate Transactions (Class 0):** 99.83%
- **Fraudulent Transactions (Class 1):** 0.17%

ğŸ”— **[Credit Card Fraud Detection Dataset]("https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/data/creditcard.csv)**

Since fraud cases are extremely rare, **handling imbalanced data** is a key part of this project.

## ğŸ” Approach & Models Used
### 1ï¸âƒ£ **Isolation Forest (Unsupervised Learning)**
- Detects fraud based on how quickly an instance is isolated in decision trees.

### 2ï¸âƒ£ **Autoencoder (Deep Learning)**
- A neural network trained to reconstruct normal transactions.
- High reconstruction error indicates possible fraud.

### 3ï¸âƒ£ **XGBoost with SMOTE (Supervised Learning)**
- **XGBoost**: A gradient boosting algorithm.
- **SMOTE**: Synthetic Minority Over-sampling Technique to balance the dataset.

## âš– Handling Imbalanced Data
Since fraud cases are highly imbalanced, the following techniques were used:
-  **SMOTE** for oversampling fraudulent cases for XGBoost.
-  **Anomaly detection models** (Isolation Forest & Autoencoder), which do not require balanced data.

## ğŸ“Š Model Evaluation
Since accuracy alone is misleading for imbalanced data, we evaluated models using:
- **Precision-Recall (PR) Curve**
- **AUC-ROC Score**
- **F1-Score**

### **Precision-Recall Curve Results:**
- **Isolation Forest AUC = 0.135**
- **Autoencoder AUC = 0.170**
- **XGBoost AUC = 0.990** âœ…

ğŸ“Š **XGBoost outperformed anomaly detection models**, making it the best choice for fraud detection!

## ğŸ” Feature Importance using SHAP
To understand which features contribute most to fraud detection, we used **SHAP values**. The top fraud indicators were:
- V4, V14, V8, and V12 have the most significant impact on fraud detection.
- Time and Amount are less influential compared to the engineered features (V1-V28).

## ğŸš€ Installation & Running the Project
### **1ï¸âƒ£ Clone the Repository**
```bash
git clone https://github.com/Victkhur/fraud-detection-project.git
cd fraud-detection-project
```

### **2ï¸âƒ£ Install Dependencies**
```bash
pip install -r requirements.txt
```

### **3ï¸âƒ£ Run the Jupyter Notebook**
```bash
jupyter notebook
```
Open the notebook and run all cells to execute the project.


## ğŸ“¢ Contributing
Feel free to contribute by submitting a pull request! ğŸ˜Š

## ğŸ“¬ Contact
If you have any questions, reach out to me on **LinkedIn** or **GitHub**!

---
ğŸš€ **Star this repository** if you find it useful! â­

